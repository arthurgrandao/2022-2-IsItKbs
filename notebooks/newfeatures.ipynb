{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "from scipy.sparse import vstack, hstack, csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_counter(string, type=None):\n",
    "    if (type == 'c'):\n",
    "        return {letter: str(string).count(letter) for letter in 'bcdfghjklmnpqrstvwxyz'}\n",
    "    elif (type == 'v'):\n",
    "        return {letter: str(string).count(letter) for letter in 'aeiou'}  \n",
    "    return {letter: str(string).count(letter) for letter in 'abcdefghijklmnopqrstuvwxyz'}\n",
    "\n",
    "def type_counter(string, type):    \n",
    "    return sum(letter_counter(string, type).values())\n",
    "\n",
    "def type_ratio(string, type):\n",
    "    if (len(string) == 0):\n",
    "        return 0\n",
    "    return type_counter(string, type)/len(string)\n",
    "\n",
    "def repetitions_character(repetitions, caractere):\n",
    "        x = repetitions.get(caractere)\n",
    "        return x if x != None else 0\n",
    "\n",
    "def ttr(string):\n",
    "    if (len(string) == 0):\n",
    "        return 0\n",
    "    ttr = len(set(string)) / len(string)\n",
    "    return ttr\n",
    "    \n",
    "def repetitions(string):\n",
    "    try:\n",
    "        resultados = []\n",
    "        size = len(string)\n",
    "        valor = 1\n",
    "\n",
    "        for l in range(0, size):\n",
    "            if l <= size-2 and string[l] == string[l+1]:\n",
    "                valor += 1\n",
    "            else:\n",
    "                resultados.append(tuple((string[l], valor)))\n",
    "                valor = 1\n",
    "        return dict(sorted(resultados))\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def repetitions_type(string, type):\n",
    "    if type == 'c':\n",
    "        letters = 'bcdfghjklmnpqrstvwxyz'\n",
    "    else:\n",
    "        letters = 'aeiou'\n",
    "    \n",
    "    resultados = []\n",
    "    size = len(string)\n",
    "    valor = 0\n",
    "\n",
    "    for l in string:\n",
    "        if l in letters:\n",
    "            valor += 1\n",
    "        else:\n",
    "            resultados.append(valor)\n",
    "            valor = 0\n",
    "    resultados.append(valor)\n",
    "    \n",
    "    return max(resultados)\n",
    "\n",
    "def bigram_counter(lista):\n",
    "  dic = {}\n",
    "  for i in lista:\n",
    "    if i in dic:\n",
    "      dic[i] +=1\n",
    "    else:\n",
    "      dic[i] =1\n",
    "  return dic\n",
    "\n",
    "def bigrams(string):\n",
    "    bigrams = []\n",
    "    for i in range(len(string)-1):\n",
    "        bigrams.append(string[i]+string[i+1])\n",
    "\n",
    "    return bigram_counter(bigrams)\n",
    "\n",
    "def bigram_max_occurance(string):\n",
    "    try:\n",
    "        return (sorted(bigrams(string).values(), reverse=True))[0]\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"..\\\\data\\\\interim\\\\naturals.csv\", \"r\", encoding=\"utf-8\") as file0:\n",
    "    X0 = pd.DataFrame(pd.read_csv(file0, keep_default_na=False))\n",
    "\n",
    "with open (\"..\\\\data\\\\interim\\\\mashings.csv\", \"r\", encoding=\"utf-8\") as file1:\n",
    "    X1 = pd.DataFrame(pd.read_csv(file1, keep_default_na=False))\n",
    "\n",
    "with open (\"..\\\\data\\\\processed\\\\rfdata.csv\", \"r\", encoding=\"utf-8\") as file1:\n",
    "    df2 = pd.DataFrame(pd.read_csv(file1, keep_default_na=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resembles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>under</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>having</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11648</th>\n",
       "      <td>It</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11649</th>\n",
       "      <td>bloom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11650</th>\n",
       "      <td>weights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11651</th>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11652</th>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11653 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0      resembles\n",
       "1        attacks\n",
       "2          under\n",
       "3         having\n",
       "4          point\n",
       "...          ...\n",
       "11648         It\n",
       "11649      bloom\n",
       "11650    weights\n",
       "11651         in\n",
       "11652        use\n",
       "\n",
       "[11653 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#O dataframe naturals é embaralhado e reduzido para o tamanho do de mashings\n",
    "X0 = shuffle(X0, random_state=777).reset_index(drop=True)\n",
    "X0 = X0[:len(X1)]\n",
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0  mash\n",
      "0      resembles     0\n",
      "1        attacks     0\n",
      "2          under     0\n",
      "3         having     0\n",
      "4          point     0\n",
      "...          ...   ...\n",
      "11648         It     0\n",
      "11649      bloom     0\n",
      "11650    weights     0\n",
      "11651         in     0\n",
      "11652        use     0\n",
      "\n",
      "[11653 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Add da coluna binária 'mash'\n",
    "X0[\"mash\"] = 0\n",
    "X1[\"mash\"] = 1\n",
    "print(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>mash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resembles</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attacks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>under</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>having</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>point</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23301</th>\n",
       "      <td>wpetqiitqyprypiy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23302</th>\n",
       "      <td>ywyeoiwtopypotpi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23303</th>\n",
       "      <td>tpoiouteiwppweiq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23304</th>\n",
       "      <td>pyiitiwiuyootpyq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23305</th>\n",
       "      <td>ieeyiewwpqtrwuwq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23306 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words  mash\n",
       "0             resembles     0\n",
       "1               attacks     0\n",
       "2                 under     0\n",
       "3                having     0\n",
       "4                 point     0\n",
       "...                 ...   ...\n",
       "23301  wpetqiitqyprypiy     1\n",
       "23302  ywyeoiwtopypotpi     1\n",
       "23303  tpoiouteiwppweiq     1\n",
       "23304  pyiitiwiuyootpyq     1\n",
       "23305  ieeyiewwpqtrwuwq     1\n",
       "\n",
       "[23306 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([X0, X1], axis=0).rename(columns={'0':'words'}, inplace=False).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23306, 153986)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract N-grams\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(2, 3), analyzer='char')\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 4), analyzer='char', stop_words=None)\n",
    "ngram_features = vectorizer.fit_transform(df2['words'])\n",
    "# ngram_features = vectorizer.fit_transform(df['words']).toarray()\n",
    "ngram_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23306, 153986)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract N-grams\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(2, 3), analyzer='char')\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 4), analyzer='char', stop_words=None)\n",
    "ngram_features = vectorizer.fit_transform(df['words'])\n",
    "# ngram_features = vectorizer.fit_transform(df['words']).toarray()\n",
    "ngram_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>mash</th>\n",
       "      <th>vowel rt</th>\n",
       "      <th>consonant rt</th>\n",
       "      <th>ttr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resembles</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attacks</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>under</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>having</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>point</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23301</th>\n",
       "      <td>wpetqiitqyprypiy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23302</th>\n",
       "      <td>ywyeoiwtopypotpi</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23303</th>\n",
       "      <td>tpoiouteiwppweiq</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23304</th>\n",
       "      <td>pyiitiwiuyootpyq</td>\n",
       "      <td>1</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23305</th>\n",
       "      <td>ieeyiewwpqtrwuwq</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23306 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words  mash  vowel rt  consonant rt       ttr\n",
       "0             resembles     0  0.333333      0.666667  0.666667\n",
       "1               attacks     0  0.285714      0.714286  0.714286\n",
       "2                 under     0  0.400000      0.600000  1.000000\n",
       "3                having     0  0.333333      0.666667  1.000000\n",
       "4                 point     0  0.400000      0.600000  1.000000\n",
       "...                 ...   ...       ...           ...       ...\n",
       "23301  wpetqiitqyprypiy     1  0.250000      0.750000  0.500000\n",
       "23302  ywyeoiwtopypotpi     1  0.375000      0.625000  0.437500\n",
       "23303  tpoiouteiwppweiq     1  0.500000      0.500000  0.500000\n",
       "23304  pyiitiwiuyootpyq     1  0.437500      0.562500  0.500000\n",
       "23305  ieeyiewwpqtrwuwq     1  0.375000      0.625000  0.562500\n",
       "\n",
       "[23306 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Análise de tamanho e razão de vogais e consoantes e de TTR\n",
    "\n",
    "aux = df.copy()\n",
    "aux[\"vowel rt\"] = aux[\"words\"].apply(lambda x : type_ratio(str(x), 'v'))\n",
    "aux[\"consonant rt\"] = aux[\"vowel rt\"].apply(lambda x : 1-x)\n",
    "aux[\"ttr\"] = aux[\"words\"].apply(ttr)\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23306, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_feature = csr_matrix(np.array(aux[\"vowel rt\"]).reshape(-1,1))\n",
    "consonant_feature = csr_matrix(np.array(aux[\"consonant rt\"]).reshape(-1,1))\n",
    "ttr_feature = csr_matrix(np.array(aux[\"ttr\"]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vowel_feature.shape: (30672, 1)\n",
      "consonant_feature.shape: (30672, 1)\n",
      "ttr_feature.shape: (30672, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"vowel_feature.shape: {vowel_feature.shape}\\nconsonant_feature.shape: {consonant_feature.shape}\\nttr_feature.shape: {ttr_feature.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23306, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_features = hstack((vowel_feature, consonant_feature, ttr_feature))\n",
    "lexical_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23306, 153989)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = hstack((lexical_features, ngram_features))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23306x153989 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 636097 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the features (optional)\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "features = scaler.fit_transform(np.abs(features))\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9995709535557225\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, df['mash'], test_size=0.4, random_state=777)\n",
    "\n",
    "# Train the classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(features_test, labels_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = clf.score(features_test, labels_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9893811005041295\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, df['mash'], test_size=0.4, random_state=777)\n",
    "\n",
    "# Train the classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = clf.score(features_test, labels_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr(\"batatetitotu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]:pneumonoultramicroscopicsilicovolcanoconiosis:[[0.62 0.38]]\n"
     ]
    }
   ],
   "source": [
    "entrada = input(\"\")\n",
    "# Extract N-gram features for the new string\n",
    "input_ngram_features = vectorizer.transform([entrada])\n",
    "\n",
    "# Extract lexical features for the new string\n",
    "input_vowel_feature = csr_matrix(np.array(type_ratio(entrada, 'v')).reshape(-1,1))\n",
    "input_consonant_feature = csr_matrix(np.array(type_ratio(entrada, 'c')).reshape(-1,1))\n",
    "input_ttr_feature = csr_matrix(np.array(ttr(entrada)).reshape(-1,1))\n",
    "\n",
    "# Combine the N-gram and lexical features into a single feature matrix\n",
    "input_lexical_features = hstack((input_vowel_feature, input_consonant_feature, input_ttr_feature))\n",
    "input_features = hstack((input_lexical_features, input_ngram_features))\n",
    "\n",
    "# Make the prediction using the trained model\n",
    "prediction = clf.predict(input_features)\n",
    "predprob = clf.predict_proba(input_features)\n",
    "# Print the prediction\n",
    "print(f\"{prediction}:{entrada}:{predprob}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0]: Incomprehensibilities:[[1.00000000e+000 1.68844634e-110]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]:yhtecwtatnelurf \n",
      "[1]:xzdagohdnioliou \n",
      "[0]:cictutbuvjnartoltitf\n",
      "[0]:ptulldegnjkruormytrq\n",
      "[0]:lnaebzbanatidiviejiu\n",
      "[0]:yatiacodlwalthrpzaao\n",
      "[0]:pruteebci\n",
      "[0]:zaembaedzc\n",
      "[0]:ihnccitws\n",
      "[0]:rondlrpe\n",
      "[0]:loeniaerkhyh\n",
      "[1]:ipippipoiopiopiopiop\n",
      "[1]:ipoiopiopipoipoiopiopiopio\n",
      "[1]:poipoiopopipioppi\n",
      "[1]:opoiopipoiopipoiop\n",
      "[1]:iopoioiopipoipoipo\n",
      "[1]:iouiuioiuouiuiouoioiu\n",
      "[1]:iouiouiouiouoiuioui\n",
      "[1]:ouiouiouiouiuyiiu\n",
      "[0]:rtyytrrytrytytrytryt\n",
      "[0]:trtrtrtrterererererererer\n",
      "[0]:ewewewewewewrerer\n",
      "[0]:ererereretrtetrtrtrytyt\n",
      "[0]:tyuituyituyuitoippoipoi\n",
      "[1]:zqmapmplmlpmlpmplpmlpmlpmlpmlpmlpmllpmlpmlpmlpmlpmlmlpmlplmpl\n",
      "[0]:c7pprmeeduss\n",
      "[0]:weustt9nneas\n",
      "[0]:tbassuahpfep\n",
      "[0]:kiraezy\n",
      "[0]:itheair\n",
      "[0]:nepsawl\n",
      "[0]:hwrcosb\n",
      "[0]:finiefj\n",
      "[0]:nmimcom\n",
      "[0]:hriphao\n",
      "[0]:xourgec\n",
      "[0]:tacimu4\n",
      "[0]:ouiulct\n",
      "[0]:nuppuno\n",
      "[0]:a8nutss\n",
      "[0]:knfelnpesroopcc\n",
      "[0]:vpieaporitfuccy\n",
      "[0]:jiaxddeuhnturarefj\n",
      "[0]:eu1cvgirtronn\n"
     ]
    }
   ],
   "source": [
    "entradas = [\"yhtecwtatnelurf \", \"xzdagohdnioliou \", \"cictutbuvjnartoltitf\", \"ptulldegnjkruormytrq\",\n",
    "         \"lnaebzbanatidiviejiu\", \"yatiacodlwalthrpzaao\", \"pruteebci\", \"zaembaedzc\", \"ihnccitws\",\n",
    "         \"rondlrpe\", \"loeniaerkhyh\", \"ipippipoiopiopiopiop\", \"ipoiopiopipoipoiopiopiopio\", \"poipoiopopipioppi\",\n",
    "         \"opoiopipoiopipoiop\", \"iopoioiopipoipoipo\", \"iouiuioiuouiuiouoioiu\", \"iouiouiouiouoiuioui\", \"ouiouiouiouiuyiiu\",\n",
    "         \"rtyytrrytrytytrytryt\", \"trtrtrtrterererererererer\", \"ewewewewewewrerer\", \"ererereretrtetrtrtrytyt\", \"tyuituyituyuitoippoipoi\",\n",
    "         \"zqmapmplmlpmlpmplpmlpmlpmlpmlpmlpmllpmlpmlpmlpmlpmlmlpmlplmpl\", \"c7pprmeeduss\", \"weustt9nneas\", \"tbassuahpfep\",\n",
    "         \"kiraezy\", \"itheair\", \"nepsawl\", \"hwrcosb\", \"finiefj\", \"nmimcom\", \"hriphao\", \"xourgec\", \"tacimu4\", \"ouiulct\",\n",
    "         \"nuppuno\", \"a8nutss\", \"knfelnpesroopcc\", \"vpieaporitfuccy\", \"jiaxddeuhnturarefj\", \"eu1cvgirtronn\"]\n",
    "for entrada in entradas:\n",
    "    # Extract N-gram features for the new string\n",
    "    input_ngram_features = vectorizer.transform([entrada])\n",
    "\n",
    "    # Extract lexical features for the new string\n",
    "    input_len_feature = csr_matrix(np.array(len(entrada)).reshape(-1,1))\n",
    "    input_vowel_feature = csr_matrix(np.array(type_ratio(entrada, 'v')).reshape(-1,1))\n",
    "    input_consonant_feature = csr_matrix(np.array(type_ratio(entrada, 'c')).reshape(-1,1))\n",
    "    input_ttr_feature = csr_matrix(np.array(ttr(entrada)).reshape(-1,1))\n",
    "\n",
    "    # Combine the N-gram and lexical features into a single feature matrix\n",
    "    input_lexical_features = hstack((input_vowel_feature, input_consonant_feature, input_ttr_feature))\n",
    "    input_features = hstack((input_lexical_features, input_ngram_features))\n",
    "\n",
    "    # Make the prediction using the trained model\n",
    "    prediction = clf.predict(input_features)\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"{prediction}:{entrada}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"..\\\\data\\\\raw\\\\words.csv\", \"r\", encoding=\"utf-8\") as file1:\n",
    "    words = pd.DataFrame(pd.read_csv(file1, keep_default_na=False))\n",
    "strwords = words.words.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calciobiotite\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dougl\\Documents\\UNB\\Sem 4\\MDS\\2022-2-Squad03\\notebooks\\newfeatures.ipynb Célula 24\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/UNB/Sem%204/MDS/2022-2-Squad03/notebooks/newfeatures.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m input_features \u001b[39m=\u001b[39m hstack((input_lexical_features, input_ngram_features))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/UNB/Sem%204/MDS/2022-2-Squad03/notebooks/newfeatures.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Make the prediction using the trained model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/UNB/Sem%204/MDS/2022-2-Squad03/notebooks/newfeatures.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m prediction \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mpredict(input_features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/UNB/Sem%204/MDS/2022-2-Squad03/notebooks/newfeatures.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Print the prediction\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/UNB/Sem%204/MDS/2022-2-Squad03/notebooks/newfeatures.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m prediction:\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:821\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    801\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 821\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[0;32m    823\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    824\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:874\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    869\u001b[0m all_proba \u001b[39m=\u001b[39m [\n\u001b[0;32m    870\u001b[0m     np\u001b[39m.\u001b[39mzeros((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], j), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m    871\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39matleast_1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[0;32m    872\u001b[0m ]\n\u001b[0;32m    873\u001b[0m lock \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mLock()\n\u001b[1;32m--> 874\u001b[0m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\n\u001b[0;32m    875\u001b[0m     delayed(_accumulate_prediction)(e\u001b[39m.\u001b[39;49mpredict_proba, X, all_proba, lock)\n\u001b[0;32m    876\u001b[0m     \u001b[39mfor\u001b[39;49;00m e \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_\n\u001b[0;32m    877\u001b[0m )\n\u001b[0;32m    879\u001b[0m \u001b[39mfor\u001b[39;00m proba \u001b[39min\u001b[39;00m all_proba:\n\u001b[0;32m    880\u001b[0m     proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_)\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:651\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[1;34m(predict, X, out, lock)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[0;32m    645\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[39m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \n\u001b[0;32m    648\u001b[0m \u001b[39m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[39m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m     prediction \u001b[39m=\u001b[39m predict(X, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    652\u001b[0m     \u001b[39mwith\u001b[39;00m lock:\n\u001b[0;32m    653\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:923\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    921\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    922\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[1;32m--> 923\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    925\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    926\u001b[0m     proba \u001b[39m=\u001b[39m proba[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_]\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:776\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:781\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dougl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:119\u001b[0m, in \u001b[0;36mspmatrix.get_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m     new_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape(shape, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39masformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat)\n\u001b[0;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39m=\u001b[39m new_matrix\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m--> 119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_shape\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    120\u001b[0m     \u001b[39m\"\"\"Get shape of a matrix.\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for entrada in strwords:\n",
    "    i += 1\n",
    "    if (i % 100 != 0):\n",
    "        continue\n",
    "    # Extract N-gram features for the new string\n",
    "    input_ngram_features = vectorizer.transform([entrada])\n",
    "\n",
    "    # Extract lexical features for the new string\n",
    "    input_vowel_feature = csr_matrix(np.array(type_ratio(entrada, 'v')).reshape(-1,1))\n",
    "    input_consonant_feature = csr_matrix(np.array(type_ratio(entrada, 'c')).reshape(-1,1))\n",
    "    input_ttr_feature = csr_matrix(np.array(ttr(entrada)).reshape(-1,1))\n",
    "\n",
    "    # Combine the N-gram and lexical features into a single feature matrix\n",
    "    input_lexical_features = hstack((input_vowel_feature, input_consonant_feature, input_ttr_feature))\n",
    "    input_features = hstack((input_lexical_features, input_ngram_features))\n",
    "\n",
    "    # Make the prediction using the trained model\n",
    "    prediction = clf.predict(input_features)\n",
    "\n",
    "    # Print the prediction\n",
    "    if prediction:\n",
    "        print(f\"{entrada}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"..\\\\data\\\\raw\\\\falsospositivos.txt\", \"r\", encoding=\"utf-8\") as file1:\n",
    "    fpos = [line.rstrip('\\n') for line in file1]\n",
    "with open (\"..\\\\data\\\\raw\\\\erros_ml_words.txt\", \"r\", encoding=\"utf-8\") as file2:\n",
    "    erros = [line.rstrip('\\n') for line in file2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for word in erros:\n",
    "    try:\n",
    "        if fpos.index(word) != ValueError:\n",
    "            print(f\"{word}\")\n",
    "    except:\n",
    "        continue;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38b444d0255ad8ae1b6474cf1705948d47c9a753211d06096071fba803b18e24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
